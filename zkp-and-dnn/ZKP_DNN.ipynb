{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AQHvPu-Ts84n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the processed Cleveland file from the UCI Heart Disease dataset and insert column names\n",
        "# Since most research focuses on the binary case of disease existence and nonexistence, the target column is binary encoded\n",
        "\n",
        "cleveland = pd.read_csv(\"processed.cleveland.data\", header=None)\n",
        "cleveland.columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
        "cleveland['target'] = cleveland['target'].apply(lambda x: 1 if x > 0 else 0)"
      ],
      "metadata": {
        "id": "tMknEq4ftBX1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleveland"
      ],
      "metadata": {
        "id": "CSIcX21ku4_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows that have missing values are dropped\n",
        "\n",
        "cleveland = cleveland.replace({'?': np.nan}).dropna().astype(float)\n",
        "cleveland"
      ],
      "metadata": {
        "id": "cW7oOuhitz_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(cleveland.drop(['target'], 1), dtype=float)\n",
        "y = np.array(cleveland['target'])"
      ],
      "metadata": {
        "id": "bdKLmDx4y2GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the dataset\n",
        "\n",
        "mean = X.mean(axis=0)\n",
        "X -= mean\n",
        "std = X.std(axis=0)\n",
        "X /= std"
      ],
      "metadata": {
        "id": "nu4k4Lx01KsN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "YAhzdqto-Wv2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep neural network definition based on the work of Safial Islam Ayon et al.\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(13, 14)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(14, 16)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(16, 16)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.layer4 = nn.Linear(16, 14)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.output = nn.Linear(14, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.layer1(x))\n",
        "        x = self.act2(self.layer2(x))\n",
        "        x = self.act3(self.layer3(x))\n",
        "        x = self.act4(self.layer4(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "W0sAGXdo7pK2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "def model_train(model, X_train, y_train, X_val, y_val):\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    n_epochs = 500\n",
        "    batch_size = 10\n",
        "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "\n",
        "    best_acc = -np.inf\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "\n",
        "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n",
        "            bar.set_description(f\"Epoch {epoch}\")\n",
        "            for start in bar:\n",
        "                X_batch = X_train[start:start+batch_size]\n",
        "                y_batch = y_train[start:start+batch_size]\n",
        "                y_pred = model(X_batch)\n",
        "                loss = loss_fn(y_pred, y_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                acc = (y_pred.round() == y_batch).float().mean()\n",
        "                bar.set_postfix(\n",
        "                    loss=float(loss),\n",
        "                    acc=float(acc)\n",
        "                )\n",
        "\n",
        "        model.eval()\n",
        "        y_pred = model(X_val)\n",
        "        acc = (y_pred.round() == y_val).float().mean()\n",
        "        acc = float(acc)\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_weights)\n",
        "    return best_acc"
      ],
      "metadata": {
        "id": "eOc3PQJW82_I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cv_scores = []\n",
        "\n",
        "for train, test in kfold.split(X, y):\n",
        "    model = DNN()\n",
        "    acc = model_train(model, X[train], y[train], X[test], y[test])\n",
        "    print(\"Accuracy: %.2f\" % acc)\n",
        "    cv_scores.append(acc)\n",
        "\n",
        "acc = np.mean(cv_scores)\n",
        "print(\"Model accuracy: %.2f%%\" % acc*100)"
      ],
      "metadata": {
        "id": "Qy-yj1Zv-kZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ezkl\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "g1bCNxPb_teR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import ezkl"
      ],
      "metadata": {
        "id": "RZoQF4faAB4e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define EZKL related file paths\n",
        "\n",
        "model_path = os.path.join('network.onnx')\n",
        "compiled_model_path = os.path.join('network.ezkl')\n",
        "pk_path = os.path.join('test.pk')\n",
        "vk_path = os.path.join('test.vk')\n",
        "settings_path = os.path.join('settings.json')\n",
        "witness_path = os.path.join('witness.json')\n",
        "data_path = os.path.join('input.json')\n",
        "cal_data_path = os.path.join('cal_data.json')"
      ],
      "metadata": {
        "id": "aBolfDRaA-en"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = X[0].reshape(1, 13)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "torch.onnx.export(model,\n",
        "                  x,\n",
        "                  model_path,\n",
        "                  export_params=True,\n",
        "                  opset_version=10,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['input'],\n",
        "                  output_names = ['output'],\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}}\n",
        "                  )\n",
        "\n",
        "data_array = ((x).detach().numpy()).reshape([-1]).tolist()\n",
        "data = dict(input_data = [data_array])\n",
        "json.dump(data, open(data_path, 'w'))\n",
        "\n",
        "cal_data = dict(input_data = X.flatten().tolist())\n",
        "json.dump(data, open(cal_data_path, 'w'))"
      ],
      "metadata": {
        "id": "rvBXkxc3BJN6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hashed visibility for the input means that only the Poseidon hash of the inputs will be known to the prover and verifier\n",
        "# Fixed visibility for the parameters means that model weights are committed to and are used for all proofs\n",
        "\n",
        "py_run_args = ezkl.PyRunArgs()\n",
        "py_run_args.input_visibility = \"hashed\"\n",
        "py_run_args.output_visibility = \"public\"\n",
        "py_run_args.param_visibility = \"fixed\"\n",
        "\n",
        "!RUST_LOG=trace\n",
        "res = ezkl.gen_settings(model_path, settings_path, py_run_args=py_run_args)\n",
        "assert res == True\n",
        "\n",
        "res = ezkl.calibrate_settings(cal_data_path, model_path, settings_path, \"resources\")"
      ],
      "metadata": {
        "id": "SsA1QHOZCqMO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
        "assert res == True"
      ],
      "metadata": {
        "id": "KjcdAuWzCxgJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Structured Reference String\n",
        "\n",
        "res = ezkl.get_srs(settings_path)"
      ],
      "metadata": {
        "id": "UOwikrbyC0S2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = ezkl.setup(\n",
        "        compiled_model_path,\n",
        "        vk_path,\n",
        "        pk_path,\n",
        "    )\n",
        "\n",
        "\n",
        "assert res == True\n",
        "assert os.path.isfile(vk_path)\n",
        "assert os.path.isfile(pk_path)\n",
        "assert os.path.isfile(settings_path)"
      ],
      "metadata": {
        "id": "CsqXXnPdC4Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the witness for the proof\n",
        "\n",
        "witness_path = os.path.join('witness.json')\n",
        "\n",
        "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
        "assert os.path.isfile(witness_path)"
      ],
      "metadata": {
        "id": "5wCDj6fXDqq-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the proof\n",
        "\n",
        "proof_path = os.path.join('proof.json')\n",
        "\n",
        "proof = ezkl.prove(\n",
        "        witness_path,\n",
        "        compiled_model_path,\n",
        "        pk_path,\n",
        "        proof_path,\n",
        "        \"single\",\n",
        "    )\n",
        "\n",
        "print(proof)\n",
        "assert os.path.isfile(proof_path)"
      ],
      "metadata": {
        "id": "VCc4uO8IDtJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the generated proof using ezkl.verify()\n",
        "\n",
        "res = ezkl.verify(\n",
        "        proof_path,\n",
        "        settings_path,\n",
        "        vk_path,\n",
        "    )\n",
        "\n",
        "assert res == True\n",
        "print(\"verified\")"
      ],
      "metadata": {
        "id": "Kq9FKSnuE1Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solc must be set to version 0.8.20\n",
        "\n",
        "!pip install solc-select\n",
        "!solc-select install 0.8.20\n",
        "!solc-select use 0.8.20\n",
        "!solc --version"
      ],
      "metadata": {
        "id": "eg2q0dWtMKQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Solidity smart contract which can be deployed to EVM compatible chains\n",
        "\n",
        "sol_code_path = os.path.join('Verifier.sol')\n",
        "abi_path = os.path.join('Verifier.abi')\n",
        "\n",
        "res = ezkl.create_evm_verifier(\n",
        "        vk_path,\n",
        "        settings_path,\n",
        "        sol_code_path,\n",
        "        abi_path\n",
        "    )\n",
        "\n",
        "assert res == True\n",
        "assert os.path.isfile(sol_code_path)"
      ],
      "metadata": {
        "id": "bO4x4pmzMQFy"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate inputs needed to perform the verification using the Solidity smart contract\n",
        "# This includes the hashed input tesnor, output of the model and the generated proof\n",
        "\n",
        "onchain_input_array = []\n",
        "\n",
        "formatted_output = \"[\"\n",
        "for i, value in enumerate(proof[\"instances\"]):\n",
        "    for j, field_element in enumerate(value):\n",
        "        onchain_input_array.append(ezkl.vecu64_to_felt(field_element))\n",
        "        formatted_output += str(onchain_input_array[-1])\n",
        "        if j != len(value) - 1:\n",
        "            formatted_output += \", \"\n",
        "    formatted_output += \"]\"\n",
        "\n",
        "print(\"proof: \", \"0x\" + proof[\"proof\"])\n",
        "print(\"instances: \", formatted_output)"
      ],
      "metadata": {
        "id": "EkAW9xZ8Mw4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the Poseidon hash for a specific model input tensor and compare it to the provided hash above\n",
        "# This is done to assure users that the model was correctly run on their specific record data/input\n",
        "# Set test_input to the desired input array containing the 13 record data values\n",
        "\n",
        "test_input = []\n",
        "\n",
        "test_tensor = torch.tensor(np.array(test_input), dtype=torch.float).reshape(1, 13)\n",
        "test_tensor = test_tensor.unsqueeze(0)\n",
        "test_tensor = test_tensor.reshape(1, 13)\n",
        "data_array = ((test_tensor).detach().numpy()).reshape([-1]).tolist()\n",
        "\n",
        "input_scale = 8\n",
        "vecu64s = []\n",
        "for i in data_array:\n",
        "    vecu64s.append(ezkl.float_to_vecu64(i, input_scale))\n",
        "\n",
        "print(\"serialized felts array\", vecu64s)\n",
        "\n",
        "hash = ezkl.poseidon_hash(vecu64s)\n",
        "print(\"hash of serialized felts array\", hash)\n",
        "\n",
        "print(\"final Poseidon hash:\", ezkl.vecu64_to_felt(hash[0]))"
      ],
      "metadata": {
        "id": "XYKX96E5yHMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}